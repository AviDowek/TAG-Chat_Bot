{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7b1e9cb",
   "metadata": {},
   "source": [
    "Web Crawler and Scraper for TAG internal WIKI.  Login uses Selenium and BeautifulSoup is used for the actual scraping.   Consists of a Web Crawler and Scraper for locating and scraping all Wiki links.     Data is locally stored in iFrames with Page description being stored in reg HTML format, so last Paragraph is stored as the page category and the iFrame as the data.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d125ccf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import html\n",
    "import requests\n",
    "import re\n",
    "\n",
    "\n",
    "#path to DB\n",
    "output_file_path=\"\"\n",
    "\n",
    "\n",
    "# URL's \n",
    "login_url =   '        \n",
    "base_url =  ''\n",
    "start_url = base_url + '/wiki'      \n",
    "\n",
    "# Set of visited URLs\n",
    "visited = set()\n",
    "\n",
    "\n",
    "# login credentials\n",
    "username = \"redacted\"\n",
    "password = \"redacted\"\n",
    "\n",
    "# browser instance\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "\n",
    "\n",
    "# Open the login page\n",
    "driver.get(login_url)\n",
    "\n",
    "# Find the username and password input fields and fill them in\n",
    "username_field = driver.find_element( By.CSS_SELECTOR, 'input[type=\"text\"]')\n",
    "password_field = driver.find_element(By.CSS_SELECTOR, 'input[type=\"password\"]')\n",
    "\n",
    "username_field.send_keys(username)\n",
    "password_field.send_keys(password)\n",
    "\n",
    "# Submit the login form\n",
    "password_field.send_keys(Keys.RETURN)\n",
    "\n",
    "# Wait for the login to complete\n",
    "wait = WebDriverWait(driver, 10)\n",
    "wait.until(EC.url_changes(login_url))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Transfer the session cookies from the Selenium driver to the requests session\n",
    "cookies = driver.get_cookies()\n",
    "session = requests.Session()\n",
    "for cookie in cookies:\n",
    "    session.cookies.set(cookie['name'], cookie['value'])\n",
    "\n",
    "\n",
    "exceptions=[\"iOS\",\"iPhone\",\"BlackBerry\",\"NetNanny\",\"mSpy\"]        \n",
    "#Add spaces between sentences.  Required for Data Cleaning    \n",
    "def add_spaces(text, exclusions):\n",
    "    \n",
    "    # Use regular expression to insert spaces before words that start with a capital letter\n",
    "    pattern = r'(?<=[a-z])(?=[A-Z])(?!(?:' + '|'.join(re.escape(ex) for ex in exceptions) + '))'\n",
    "    modified_text = re.sub(pattern, ' ', text)\n",
    "    return modified_text    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "#Function for crawling and scraping all Wiki Sites\n",
    "def scrape_page(url):\n",
    "    \n",
    "   \n",
    "    \n",
    "    response = session.get(url)\n",
    "    if response.status_code == 200:   \n",
    "        visited.add(url)\n",
    "        \n",
    "        # Parse the page using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Find the iframe and Title element\n",
    "        iframe_data = soup.find_all(\"iframe\")\n",
    "        header=soup.find_all('p')[-1]\n",
    "        \n",
    "        \n",
    "        with open(output_file_path, \"a\", encoding=\"utf-8\") as output_file:\n",
    "            \n",
    "        #Store the page Title (looks for last 'p' element)\n",
    "             for headers in header:\n",
    "              \n",
    "                output_file.write('\\n\\n** Data from the '+ headers.get_text()+' page **\\n')\n",
    "                \n",
    "        # Loop through the locally stored iframe elements\n",
    "             for iframe in iframe_data:\n",
    "                srcdoc_content = iframe[\"srcdoc\"]\n",
    "                decoded_content = html.unescape(srcdoc_content)\n",
    "                iframe_soup = BeautifulSoup(decoded_content, \"html.parser\")\n",
    "            \n",
    "            \n",
    "        # Store the content of the iframe\n",
    "              \n",
    "                output_file.write(add_spaces(iframe_soup.get_text(),exceptions))\n",
    "                output_file.write(\"-\" * 20)\n",
    "              \n",
    "\n",
    "    # Find links on the page and follow them\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            next_url = link['href']\n",
    "            \n",
    "            #Check that the link is local and part of the wiki\n",
    "            if 'wiki' in link.text.lower() or 'wiki' in next_url.lower():\n",
    "                if next_url.startswith('/'):  # Relative URLs\n",
    "                        next_url = base_url + next_url\n",
    "                        \n",
    "                        #If not yet scraped, scrape\n",
    "                if next_url.startswith(base_url) and next_url not in visited:\n",
    "                        scrape_page(next_url)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Start scraping from the main URL\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    scrape_page(start_url)\n",
    "    \n",
    " # Once finished,end Selenium session \n",
    "driver.quit()\n",
    "\n",
    "print('Done!')\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4a80fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9330ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
